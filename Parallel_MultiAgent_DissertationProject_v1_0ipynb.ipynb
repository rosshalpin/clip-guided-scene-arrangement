{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parallel_MultiAgent_DissertationProject_v1.0ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyMBwjo3nmGD57mEKzCa6utu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosshalpin/clip-guided-scene-arrangement/blob/main/Parallel_MultiAgent_DissertationProject_v1_0ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oL-QoqQCQ092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51c4df0-aff4-49c1-de72-5d68034be593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import clip\n",
        "  # import stable_baselines3\n",
        "  # import sb3_contrib\n",
        "  import pettingzoo\n",
        "  from plot_image_grid import image_grid\n",
        "  import supersuit as ss\n",
        "  import ray\n",
        "except ModuleNotFoundError:\n",
        "  !pip install gym==0.22.0\n",
        "  !pip install git+https://github.com/openai/CLIP.git\n",
        "  # !pip install stable-baselines3[extra]\n",
        "  # !pip install git+https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "  !pip install pettingzoo==1.19.0\n",
        "  !wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
        "  !pip install supersuit==3.3.4\n",
        "  !pip install ray\n",
        "  !pip install lz4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ved3Y9Jty3pZ",
        "outputId": "621b9fc6-a373-4c57-cf09-beda8b130faa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: gym\n",
            "Version: 0.22.0\n",
            "Summary: Gym: A universal API for reinforcement learning environments.\n",
            "Home-page: https://github.com/openai/gym\n",
            "Author: Gym Community\n",
            "Author-email: jkterry@umd.edu\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: cloudpickle, importlib-metadata, gym-notices, numpy\n",
            "Required-by: SuperSuit, PettingZoo, dopamine-rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "\n",
        "!ln -s /content/drive/My\\ Drive/Colab\\ Notebooks/ $nb_path\n",
        "\n",
        "sys.path.insert(0,nb_path)\n",
        "\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNNQl2YNQ_vp",
        "outputId": "2696add7-74d9-4f76-8a90-0686010ae792"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch3d"
      ],
      "metadata": {
        "id": "W7poN1-mRCVQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Util function for loading meshes\n",
        "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
        "from pytorch3d.ops import sample_points_from_meshes\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Meshes, join_meshes_as_batch, join_meshes_as_scene, Pointclouds\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    FoVPerspectiveCameras, \n",
        "    PointLights,\n",
        "    AmbientLights,\n",
        "    DirectionalLights, \n",
        "    Materials, \n",
        "    RasterizationSettings, \n",
        "    MeshRenderer, \n",
        "    MeshRasterizer,  \n",
        "    SoftPhongShader,\n",
        "    TexturesUV,\n",
        "    TexturesVertex\n",
        ")\n",
        "\n",
        "# add path for demo utils functions \n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(''))\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "g50aVEpDREv9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set paths\n",
        "DATA_DIR = '/content/drive/My Drive/DissertationProject_v0.0/data'"
      ],
      "metadata": {
        "id": "KJ1deW1eSbts"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mesh(input_path, dev) -> Meshes:\n",
        "  obj_filename = os.path.join(DATA_DIR, input_path)\n",
        "  return load_objs_as_meshes([obj_filename], device=dev)"
      ],
      "metadata": {
        "id": "dVuSYDaXSd-x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size"
      ],
      "metadata": {
        "id": "i-1gpPu3SiHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639dec74-a5f4-4fa1-c0f0-211233abd32f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:10<00:00, 34.3MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "from pytorch3d.renderer import (\n",
        "  HardPhongShader\n",
        ")\n",
        "\n",
        "class SceneObject():\n",
        "    def __init__(self, mesh, scale=1):\n",
        "      new_mesh = mesh.clone().scale_verts(scale)\n",
        "      self._mesh = new_mesh\n",
        "      self._scale = scale\n",
        "      self._position = self._mesh_position()\n",
        "      self._prev_position = self._mesh_position()\n",
        "\n",
        "    @property\n",
        "    def mesh(self):\n",
        "      return self._mesh\n",
        "\n",
        "    @property\n",
        "    def position(self):\n",
        "      return self._position\n",
        "\n",
        "    @position.setter\n",
        "    def position(self, value):\n",
        "      offset = [round(a-b,3) for a, b in zip(value, self._position)]\n",
        "      self._set_position_helper(offset)\n",
        "\n",
        "    def _mesh_position(self):\n",
        "      return [round(float(((c.cpu()[0]+c.cpu()[1])/2)), 3) for c in self._mesh.get_bounding_boxes()[0]]\n",
        "\n",
        "    def _set_position_helper(self, value):\n",
        "      self._prev_position = copy.deepcopy(self._position)\n",
        "      offset = self._mesh.verts_padded().new_tensor(value).expand(self._mesh.verts_packed().shape)\n",
        "      self._mesh = self._mesh.offset_verts(offset)\n",
        "      self._position = self._mesh_position()\n",
        "\n",
        "    def translate(self, value):\n",
        "      self._set_position_helper(value)\n",
        "\n",
        "    def reset_pos(self):\n",
        "      self._position = copy.deepcopy(self._prev_position)\n",
        "\n",
        "class Scene():\n",
        "  def __init__(self, meshes: list, azim, elev, dist):\n",
        "    self.AZIM = azim\n",
        "    self.ELEV = elev\n",
        "    self.num_cameras = max(len(self.AZIM), len(self.ELEV))\n",
        "    self._meshes = meshes\n",
        "    self.CAMERA_DIST = dist\n",
        "    self._scene = join_meshes_as_scene(meshes).extend(self.num_cameras)\n",
        "    self.device = device\n",
        "    \n",
        "\n",
        "  @property\n",
        "  def scene(self):\n",
        "    return self._scene\n",
        "\n",
        "  @scene.setter\n",
        "  def scene(self, value):\n",
        "    self._scene = join_meshes_as_scene(value).extend(self.num_cameras)\n",
        "\n",
        "  @property\n",
        "  def _lights(self):\n",
        "    return PointLights(device=device, location=[[0.0, 5.0, 7.0]])\n",
        "    # return AmbientLights(device=self.device)\n",
        "\n",
        "  @property\n",
        "  def _cameras(self):\n",
        "    R, T = look_at_view_transform(dist=self.CAMERA_DIST, azim=self.AZIM, elev=self.ELEV)\n",
        "    return FoVPerspectiveCameras(device=self.device, R=R, T=T)\n",
        "\n",
        "  @property\n",
        "  def renderer(self):\n",
        "    return MeshRenderer(\n",
        "        rasterizer=MeshRasterizer(\n",
        "            raster_settings=RasterizationSettings(\n",
        "              image_size=256, \n",
        "              faces_per_pixel=1,\n",
        "              bin_size=None\n",
        "            )\n",
        "        ),\n",
        "        shader=HardPhongShader(\n",
        "            device=self.device\n",
        "        )\n",
        "    )\n",
        "  \n",
        "  def render(self):\n",
        "    return self.renderer(self.scene, cameras=self._cameras, lights=self._lights).cpu().numpy()\n"
      ],
      "metadata": {
        "id": "2L7bxBeyVZzG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_pil_image(input):\n",
        "  return Image.fromarray((input * 255).astype('uint8'))\n",
        "\n",
        "def clip_sim_3(input: list, description: str):\n",
        "  text = clip.tokenize(description).to(device)\n",
        "  with torch.no_grad():\n",
        "    text_features = model.encode_text(text)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  similarities = []\n",
        "  for image_input in input:\n",
        "    # image_input = get_pil_image(image_input[0, ..., :3])\n",
        "    image_input = preprocess(image_input).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "    # print(similarity[0][0])\n",
        "    similarities.append(similarity[0][0])\n",
        "  return similarities"
      ],
      "metadata": {
        "id": "_xew_8T5VVSB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "DIRECTIONS = list(product(range(-1, 2), repeat=3))\n",
        "mod = 0.2\n",
        "ALL_DIRECTIONS = [[a * mod for a in b] for b in DIRECTIONS]\n",
        "ACTIONS_MAP = {\n",
        "  i: ALL_DIRECTIONS[i] for i in range(len(ALL_DIRECTIONS))\n",
        "}"
      ],
      "metadata": {
        "id": "dPf3y_LxmhOp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from gym.spaces import Box, Discrete, Dict, MultiDiscrete\n",
        "from pettingzoo import ParallelEnv\n",
        "from pettingzoo.utils import wrappers\n",
        "from pettingzoo.utils import parallel_to_aec\n",
        "from scipy.stats import mannwhitneyu\n",
        "from collections import OrderedDict\n",
        "\n",
        "class RenderEnv(ParallelEnv):\n",
        "  metadata = {\"render_modes\": [\"human\"], \"name\": \"render_v2\"}\n",
        "\n",
        "  def __init__(self, objs, guide, limit=None):\n",
        "    \"\"\"\n",
        "    The init method takes in environment arguments and should define the following attributes:\n",
        "    - possible_agents\n",
        "    - action_spaces\n",
        "    - observation_spaces\n",
        "\n",
        "    These attributes should not be changed after initialization.\n",
        "    \"\"\"\n",
        "    self.GUIDE_STRING = guide\n",
        "\n",
        "    self.limit = limit\n",
        "    self.rounds = 0\n",
        "\n",
        "    self.camera_config = {\n",
        "      'azim': torch.linspace(0, 180, 4),\n",
        "      'elev': [50],\n",
        "      'dist': 20.0\n",
        "    }\n",
        "\n",
        "    self.actions_map = ACTIONS_MAP\n",
        "\n",
        "    self.limit_box = [[-10,-1,-10],[10,10,10]]\n",
        "    self.p_threshold = 0.1\n",
        "    \n",
        "\n",
        "    self.best = {}\n",
        "    self.images = None\n",
        "\n",
        "    self.possible_agents = [\"object_\" + str(r) for r in range(len(objs))]\n",
        "    self.agents = self.possible_agents\n",
        "    self.agent_mapping = dict(\n",
        "        zip(self.agents, [copy.deepcopy(obj) for obj in objs])\n",
        "    )\n",
        "\n",
        "    self.scene = Scene(\n",
        "      meshes=[a.mesh for a in list(self.agent_mapping.values())], \n",
        "      **self.camera_config\n",
        "    )\n",
        "\n",
        "    self.num_cameras = self.scene.num_cameras\n",
        "    self.best_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "    self.prev_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "\n",
        "    self.limited = np.ones((len(self.agents))).astype(np.float32)\n",
        "\n",
        "    self.action_spaces = {agent: Discrete(len(self.actions_map)) for agent in self.possible_agents}\n",
        "    self.observation_spaces = {\n",
        "      agent: Box(low=-20, high=20, shape=(3,)) for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # self.action_space = {agent: Discrete(len(self.actions_map)) for agent in self.possible_agents}\n",
        "    # self.observation_space = {\n",
        "    #     agent: Discrete(len(self.actions_map)) for agent in self.possible_agents\n",
        "    # }\n",
        "\n",
        "  # this cache ensures that same space object is returned for the same agent\n",
        "  # allows action space seeding to work as expected\n",
        "\n",
        "  @functools.lru_cache(maxsize=None)\n",
        "  def observation_space(self, agent):\n",
        "    # Gym spaces are defined and documented here: https://gym.openai.com/docs/#spaces\n",
        "    return self.observation_spaces[agent]\n",
        "\n",
        "  @functools.lru_cache(maxsize=None)\n",
        "  def action_space(self, agent):\n",
        "    return self.action_spaces[agent]\n",
        "\n",
        "  def render_scene(self) -> None:\n",
        "      self.scene = Scene([a.mesh for a in list(self.agent_mapping.values())], **self.camera_config)\n",
        "      self.images = self.scene.render()\n",
        "\n",
        "  def clip_scores(self):\n",
        "    self.render_scene()\n",
        "    pil_images = [get_pil_image(img[..., :3]) for img in self.images]\n",
        "    return clip_sim_3(pil_images, self.GUIDE_STRING)\n",
        "\n",
        "  def limit_action(self, action, i):\n",
        "    limited = False\n",
        "    translation_result = [a+b for a,b in zip (list(self.agent_mapping.values())[i].position, action)]\n",
        "    for i, val in enumerate(translation_result):\n",
        "        if val < self.limit_box[0][i]:\n",
        "            limited = True\n",
        "        elif val > self.limit_box[1][i]:\n",
        "            limited = True\n",
        "    return limited\n",
        "\n",
        "  def perform_test(self, a, b):\n",
        "    stat, p = mannwhitneyu(a, b, alternative='greater',method='exact')\n",
        "    return stat, p\n",
        "\n",
        "  def get_reward(self, sim_matrix) -> int:\n",
        "    rw = 0\n",
        "\n",
        "    stat_best, p_best = self.perform_test(sim_matrix, self.best_sim_matrix)\n",
        "    stat_prev, p_prev = self.perform_test(sim_matrix, self.prev_sim_matrix)\n",
        "    # tmax = 20\n",
        "    # tmin = 0\n",
        "    # stat_best = (stat_best - tmin)/(tmax-tmin)\n",
        "    # stat_prev = (stat_prev - tmin)/(tmax-tmin)\n",
        "\n",
        "    if p_best <= self.p_threshold:\n",
        "      self.best_sim_matrix = sim_matrix\n",
        "      self.best[\"images\"] = self.images\n",
        "      self.best[\"scene\"] = self.scene.scene\n",
        "\n",
        "    rw += 1-p_best\n",
        "    rw += 1-p_prev\n",
        "\n",
        "    rw = (2 *(rw - -2)/(2- -2)) - 1\n",
        "\n",
        "    self.prev_sim_matrix = sim_matrix\n",
        "\n",
        "    return rw\n",
        "\n",
        "\n",
        "  def take_action(self, i, action):\n",
        "    value = action\n",
        "    action = self.actions_map[action]\n",
        "    limits = self.limit_action(action[:], i)\n",
        "    if limits:\n",
        "      self.limited[i] = 0.0\n",
        "    else:\n",
        "      self.agent_mapping[self.agents[i]].translate(action)\n",
        "\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    \"\"\"\n",
        "    Reset needs to initialize the `agents` attribute and must set up the\n",
        "    environment so that render(), and step() can be called without issues.\n",
        "\n",
        "    Here it initializes the `num_moves` variable which counts the number of\n",
        "    hands that are played.\n",
        "\n",
        "    Returns the observations for each agent\n",
        "    \"\"\"\n",
        "    self.limited = np.ones((len(self.agents))).astype(np.float32)\n",
        "    self.best_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "    self.prev_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "    self.rounds = 0\n",
        "    return {agent: np.asarray([0,0,0]).astype(np.float32) for agent in self.possible_agents}\n",
        "\n",
        "  def step(self, actions):\n",
        "    \"\"\"\n",
        "    step(action) takes in an action for each agent and should return the\n",
        "    - observations\n",
        "    - rewards\n",
        "    - dones\n",
        "    - infos\n",
        "    dicts where each dict looks like {agent_1: item_1, agent_2: item_2}\n",
        "    \"\"\"\n",
        "    done = False\n",
        "    self.rounds +=1 \n",
        "\n",
        "    # If a user passes in actions with no agents, then just return empty observations, etc.\n",
        "    if not actions:\n",
        "        self.agents = []\n",
        "        return {}, {}, {}, {}\n",
        "\n",
        "    if self.limit != None:\n",
        "      env_done = self.rounds >= self.limit\n",
        "    else:\n",
        "      env_done = False\n",
        "    dones = {agent: env_done for agent in self.agents}\n",
        "\n",
        "    for i in range(len(self.possible_agents)):\n",
        "      self.take_action(i, actions[self.agents[i]])\n",
        "\n",
        "    # current observation is just the other player's most recent action\n",
        "    observations = {agent: self.agent_mapping[agent].position  for agent in self.possible_agents}\n",
        "\n",
        "    sim_matrix = np.asarray(self.clip_scores()).astype(np.float32)\n",
        "\n",
        "    overall_reward = self.get_reward(sim_matrix)\n",
        "\n",
        "    rewards = {self.agents[i]: overall_reward * self.limited[i] for i in range(len(self.agents)) }\n",
        "\n",
        "    self.limited = np.ones((len(self.agents))).astype(np.float32)\n",
        "    # typically there won't be any information in the infos, but there must\n",
        "    # still be an entry for each agent\n",
        "    infos = {agent: {} for agent in self.agents}\n",
        "\n",
        "    # infos[\"images\"] = self.images\n",
        "    # infos[\"best\"] = self.best\n",
        "\n",
        "    if env_done:\n",
        "        self.agents = []\n",
        "\n",
        "    return observations, rewards, dones, infos"
      ],
      "metadata": {
        "id": "IANKoqv-UWUt",
        "cellView": "code"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_mesh = load_mesh(f\"{DATA_DIR}/fruit_mesh/pear_export.obj\", torch.device(\"cuda\"))\n",
        "table_mesh = load_mesh(f\"{DATA_DIR}/table_mesh/GenericClassicTable001.obj\", torch.device(\"cuda\"))"
      ],
      "metadata": {
        "id": "3wlHmzAVSgC2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_object = SceneObject(fruit_mesh, scale=0.25)\n",
        "fruit_object2 = SceneObject(fruit_mesh, scale=0.25)\n",
        "table_object = SceneObject(table_mesh, scale=8)\n",
        "table_object.position=[0,0,0]"
      ],
      "metadata": {
        "id": "OjHDJAvyKphZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RenderEnv([fruit_object,fruit_object2, table_object], \"Pieces of fruit on top of a wooden table\")\n",
        "max_cycles = 2\n",
        "\n",
        "for step in range(max_cycles):\n",
        "  actions = {agent: Discrete(len(env.actions_map)).sample() for agent in env.agents}\n",
        "  observations, rewards, dones, infos = env.step(actions)\n",
        "  print(observations)\n",
        "env.reset()"
      ],
      "metadata": {
        "id": "P7GjBLgrKAyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66b8aff-d7d5-4915-ee88-4e76a33e0bdc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'object_0': [0.184, 0.436, -0.201], 'object_1': [-0.216, 0.236, 0.199], 'object_2': [-0.2, 0.2, -0.2]}\n",
            "{'object_0': [-0.016, 0.636, -0.401], 'object_1': [-0.416, 0.236, -0.001], 'object_2': [0.0, 0.4, -0.0]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'object_0': array([0., 0., 0.], dtype=float32),\n",
              " 'object_1': array([0., 0., 0.], dtype=float32),\n",
              " 'object_2': array([0., 0., 0.], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7L6vLQlQo9bd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_grid(env.best[\"images\"], rows=1, cols=4, rgb=True)"
      ],
      "metadata": {
        "id": "fwlHe9GYTJjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_batch_individually(env.scene.scene[0])"
      ],
      "metadata": {
        "id": "0FmrFMB5o2dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import supersuit as ss\n",
        "# from stable_baselines3 import PPO, A2C\n",
        "\n",
        "# env = RenderEnv([fruit_object,fruit_object2, table_object], \"Pieces of fruit on top of a wooden table\")\n",
        "\n",
        "# vec_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "\n",
        "# n_envs = 3\n",
        "\n",
        "# parallel_env = ss.concat_vec_envs_v1(vec_env, n_envs, num_cpus=1, base_class='stable_baselines3')\n",
        "\n",
        "\n",
        "# n_steps = 64\n",
        "# total_timesteps = (n_steps * n_envs) * 3\n",
        "# train_model = PPO('MlpPolicy', parallel_env, verbose=1, n_steps=n_steps).learn(n_eval_episodes=64, total_timesteps=total_timesteps)"
      ],
      "metadata": {
        "id": "G-XO2_uSaaGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "cuda.empty_cache()"
      ],
      "metadata": {
        "id": "LmR8WvMgQCuW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "import ray.rllib.agents.ppo as ppo\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c9nnWymp3Nz",
        "outputId": "1d43c9d2-e31e-40f1-8085-fccade781012"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RayContext(dashboard_url='', python_version='3.7.13', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '172.28.0.2', 'raylet_ip_address': '172.28.0.2', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-08-20_10-34-43_496547_90/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-08-20_10-34-43_496547_90/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-08-20_10-34-43_496547_90', 'metrics_export_port': 45327, 'gcs_address': '172.28.0.2:64980', 'address': '172.28.0.2:64980', 'node_id': 'c14397e05d1b0843c669ad541546cf340b5f9aa5e81594a50dbd58d0'})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}