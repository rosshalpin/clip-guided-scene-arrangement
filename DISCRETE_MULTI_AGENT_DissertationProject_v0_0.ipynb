{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DISCRETE_MULTI_AGENT_DissertationProject_v0.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyOPDokyY26zae0pfeen33+f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosshalpin/clip-guided-scene-arrangement/blob/main/DISCRETE_MULTI_AGENT_DissertationProject_v0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import clip\n",
        "  import stable_baselines3\n",
        "  import sb3_contrib\n",
        "  import pettingzoo\n",
        "  from plot_image_grid import image_grid\n",
        "  import supersuit as ss\n",
        "  import optuna\n",
        "except (ModuleNotFoundError, ImportError):\n",
        "  !pip install git+https://github.com/openai/CLIP.git\n",
        "  !pip install stable-baselines3[extra]\n",
        "  !pip install git+https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "  !pip install pettingzoo\n",
        "  !wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
        "  !pip install supersuit\n",
        "  !pip install optuna"
      ],
      "metadata": {
        "id": "y9M-JuIbRW3r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "\n",
        "!ln -s /content/drive/My\\ Drive/Colab\\ Notebooks/ $nb_path\n",
        "\n",
        "sys.path.insert(0,nb_path)\n",
        "\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6NnZU63RZhp",
        "outputId": "76f2cce8-c91a-48e3-b1d1-002ba27ae43a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/notebooks/Colab Notebooks': File exists\n",
            "ln: failed to create symbolic link '/mydrive': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch3d\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Util function for loading meshes\n",
        "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
        "from pytorch3d.ops import sample_points_from_meshes\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Meshes, join_meshes_as_batch, join_meshes_as_scene, Pointclouds\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    FoVPerspectiveCameras, \n",
        "    PointLights,\n",
        "    AmbientLights,\n",
        "    DirectionalLights, \n",
        "    Materials, \n",
        "    RasterizationSettings, \n",
        "    MeshRenderer, \n",
        "    MeshRasterizer,  \n",
        "    SoftPhongShader,\n",
        "    TexturesUV,\n",
        "    TexturesVertex\n",
        ")\n",
        "\n",
        "# add path for demo utils functions \n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(''))\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "K-pxQlPSRmQG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set paths\n",
        "DATA_DIR = '/content/drive/My Drive/DissertationProject_v0.0/data'\n",
        "\n",
        "def load_mesh(input_path) -> Meshes:\n",
        "  obj_filename = os.path.join(DATA_DIR, input_path)\n",
        "  return load_objs_as_meshes([obj_filename], device=device)"
      ],
      "metadata": {
        "id": "R85GNylTRn7B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size"
      ],
      "metadata": {
        "id": "922F32PWRtLi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "from pytorch3d.renderer import (\n",
        "  HardPhongShader\n",
        ")\n",
        "\n",
        "class SceneObject():\n",
        "    def __init__(self, mesh, scale=1):\n",
        "      new_mesh = mesh.clone().scale_verts(scale)\n",
        "      self._mesh = new_mesh\n",
        "      self._scale = scale\n",
        "      self._position = self._mesh_position()\n",
        "      self._prev_position = self._mesh_position()\n",
        "\n",
        "    @property\n",
        "    def mesh(self):\n",
        "      return self._mesh\n",
        "\n",
        "    @property\n",
        "    def position(self):\n",
        "      return self._position\n",
        "\n",
        "    @position.setter\n",
        "    def position(self, value):\n",
        "      # print(value, self._position)\n",
        "      offset = [round(a-b,3) for a, b in zip(value, self._position)]\n",
        "      self._set_position_helper(offset)\n",
        "\n",
        "    def _mesh_position(self):\n",
        "      return [round(float(((c.cpu()[0]+c.cpu()[1])/2)), 3) for c in self._mesh.get_bounding_boxes()[0]]\n",
        "\n",
        "    def _set_position_helper(self, value):\n",
        "      self._prev_position = copy.deepcopy(self._position)\n",
        "      offset = self._mesh.verts_padded().new_tensor(value).expand(self._mesh.verts_packed().shape)\n",
        "      self._mesh = self._mesh.offset_verts(offset)\n",
        "      self._position = self._mesh_position()\n",
        "\n",
        "    def translate(self, value):\n",
        "      self._set_position_helper(value)\n",
        "\n",
        "    def reset_pos(self):\n",
        "      self._position = copy.deepcopy(self._prev_position)\n",
        "\n",
        "class Scene():\n",
        "  def __init__(self, meshes: list, azim, elev, dist):\n",
        "    self.AZIM = azim\n",
        "    self.ELEV = elev\n",
        "    self.num_cameras = max(len(self.AZIM), len(self.ELEV))\n",
        "    self._meshes = meshes\n",
        "    self.CAMERA_DIST = dist\n",
        "    self._scene = join_meshes_as_scene(meshes).extend(self.num_cameras)\n",
        "    self.device = device\n",
        "    \n",
        "\n",
        "  @property\n",
        "  def scene(self):\n",
        "    return self._scene\n",
        "\n",
        "  @scene.setter\n",
        "  def scene(self, value):\n",
        "    self._scene = join_meshes_as_scene(value).extend(self.num_cameras)\n",
        "\n",
        "  @property\n",
        "  def _lights(self):\n",
        "    return PointLights(device=device, location=[[0.0, 5.0, 7.0]])\n",
        "    # return AmbientLights(device=self.device)\n",
        "\n",
        "  @property\n",
        "  def _cameras(self):\n",
        "    R, T = look_at_view_transform(dist=self.CAMERA_DIST, azim=self.AZIM, elev=self.ELEV)\n",
        "    return FoVPerspectiveCameras(device=self.device, R=R, T=T)\n",
        "\n",
        "  @property\n",
        "  def renderer(self):\n",
        "    return MeshRenderer(\n",
        "        rasterizer=MeshRasterizer(\n",
        "            raster_settings=RasterizationSettings(\n",
        "              image_size=256, \n",
        "              faces_per_pixel=1,\n",
        "              bin_size=None\n",
        "            )\n",
        "        ),\n",
        "        shader=HardPhongShader(\n",
        "            device=self.device\n",
        "        )\n",
        "    )\n",
        "  \n",
        "  def render(self):\n",
        "    return self.renderer(self.scene, cameras=self._cameras, lights=self._lights).cpu().numpy()\n"
      ],
      "metadata": {
        "id": "VCTzUIYhRvNt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_pil_image(input):\n",
        "  return Image.fromarray((input * 255).astype('uint8'))\n",
        "\n",
        "def clip_sim_3(input: list, description: str):\n",
        "  text = clip.tokenize(description).to(device)\n",
        "  with torch.no_grad():\n",
        "    text_features = model.encode_text(text)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  similarities = []\n",
        "  for image_input in input:\n",
        "    # image_input = get_pil_image(image_input[0, ..., :3])\n",
        "    image_input = preprocess(image_input).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "    # print(similarity[0][0])\n",
        "    similarities.append(similarity[0][0])\n",
        "  return similarities"
      ],
      "metadata": {
        "id": "AJIXswBHRxaU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "DIRECTIONS = list(product(range(-1, 2), repeat=3))\n",
        "mod = 0.2\n",
        "ALL_DIRECTIONS = [[a * mod for a in b] for b in DIRECTIONS]\n",
        "ACTIONS_MAP = {\n",
        "  i: ALL_DIRECTIONS[i] for i in range(len(ALL_DIRECTIONS))\n",
        "}"
      ],
      "metadata": {
        "id": "r5IFUMHZRUtr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QBsJofnIRKwJ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "from gym.spaces import Box, Discrete\n",
        "import numpy as np\n",
        "import functools\n",
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils import agent_selector\n",
        "from pettingzoo.utils import wrappers\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "class RenderEnv(AECEnv):\n",
        "  \"\"\"\n",
        "  The metadata holds environment constants. From gym, we inherit the \"render_modes\",\n",
        "  metadata which specifies which modes can be put into the render() method.\n",
        "  At least human mode should be supported.\n",
        "  The \"name\" metadata allows the environment to be pretty printed.\n",
        "  \"\"\"\n",
        "\n",
        "  metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
        "\n",
        "  def __init__(self, objs, guide, limit=None):\n",
        "    \"\"\"\n",
        "    The init method takes in environment arguments and\n",
        "      should define the following attributes:\n",
        "    - possible_agents\n",
        "    - action_spaces\n",
        "    - observation_spaces\n",
        "\n",
        "    These attributes should not be changed after initialization.\n",
        "    \"\"\"\n",
        "    self.GUIDE_STRING = guide\n",
        "\n",
        "    self.limit = limit\n",
        "    self.rounds = 0\n",
        "\n",
        "    self.camera_config = {\n",
        "      'azim': torch.linspace(0, 180, 4),\n",
        "      'elev': [50],\n",
        "      'dist': 20.0\n",
        "    }\n",
        "\n",
        "    self.actions_map = ACTIONS_MAP\n",
        "\n",
        "    self.limit_box = [[-10,-1,-10],[10,10,10]]\n",
        "    self.p_threshold = 0.1\n",
        "    \n",
        "\n",
        "    self.best = {}\n",
        "    self.images = None\n",
        "\n",
        "    \n",
        "    self.possible_agents = [\"object_\" + str(r) for r in range(len(objs))]\n",
        "    self.agent_mapping = dict(\n",
        "        zip(self.possible_agents, [copy.deepcopy(obj) for obj in objs])\n",
        "    )\n",
        "\n",
        "    self.limited = np.ones((len(self.possible_agents))).astype(np.float32)\n",
        "\n",
        "    self.scene = Scene(\n",
        "      meshes=[a.mesh for a in list(self.agent_mapping.values())], \n",
        "      **self.camera_config\n",
        "    )\n",
        "\n",
        "    self.num_cameras = self.scene.num_cameras\n",
        "    self.best_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "    self.prev_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "\n",
        "    # Gym spaces are defined and documented here: https://gym.openai.com/docs/#spaces\n",
        "    self._action_spaces = {agent: Discrete(len(self.actions_map)) for agent in self.possible_agents}\n",
        "    self._observation_spaces = {\n",
        "        agent: Box(low=-1, high=1, shape=(3,)) for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "  # this cache ensures that same space object is returned for the same agent\n",
        "  # allows action space seeding to work as expected\n",
        "  @functools.lru_cache(maxsize=None)\n",
        "  def observation_space(self, agent):\n",
        "    # Gym spaces are defined and documented here: https://gym.openai.com/docs/#spaces\n",
        "    return Discrete(len(self.actions_map))\n",
        "\n",
        "  @functools.lru_cache(maxsize=None)\n",
        "  def action_space(self, agent):\n",
        "    return Discrete(len(self.actions_map))\n",
        "\n",
        "\n",
        "  def render_scene(self) -> None:\n",
        "      self.scene = Scene([a.mesh for a in list(self.agent_mapping.values())], **self.camera_config)\n",
        "      self.images = self.scene.render()\n",
        "\n",
        "  def clip_scores(self):\n",
        "    self.render_scene()\n",
        "    pil_images = [get_pil_image(img[..., :3]) for img in self.images]\n",
        "    return clip_sim_3(pil_images, self.GUIDE_STRING)\n",
        "\n",
        "\n",
        "  def limit_action(self, action, i):\n",
        "    limited = False\n",
        "    translation_result = [a+b for a,b in zip (list(self.agent_mapping.values())[i].position, action)]\n",
        "    for i, val in enumerate(translation_result):\n",
        "        if val < self.limit_box[0][i]:\n",
        "            limited = True\n",
        "        elif val > self.limit_box[1][i]:\n",
        "            limited = True\n",
        "    return limited\n",
        "\n",
        "  def perform_test(self, a, b):\n",
        "    stat, p = mannwhitneyu(a, b, alternative='greater',method='exact')\n",
        "    return stat, p\n",
        "\n",
        "  def get_reward(self, sim_matrix) -> int:\n",
        "    rw = 0\n",
        "\n",
        "    stat_best, p_best = self.perform_test(sim_matrix, self.best_sim_matrix)\n",
        "    stat_prev, p_prev = self.perform_test(sim_matrix, self.prev_sim_matrix)\n",
        "\n",
        "    if p_best <= self.p_threshold:\n",
        "      self.best_sim_matrix = sim_matrix\n",
        "      self.best[\"images\"] = self.images\n",
        "      self.best[\"scene\"] = self.scene.scene\n",
        "\n",
        "    rw += 1-p_best\n",
        "    rw += 1-p_prev\n",
        "\n",
        "    rw = (2 *(rw - -2)/(2- -2)) - 1\n",
        "\n",
        "    self.prev_sim_matrix = sim_matrix\n",
        "\n",
        "    return rw\n",
        "\n",
        "\n",
        "  def take_action(self, i, action):\n",
        "    action = self.actions_map[action]\n",
        "    got_limited = self.limit_action(action[:], i)\n",
        "    if got_limited:\n",
        "      self.limited[i] = 0.0\n",
        "    else:\n",
        "      self.agent_mapping[self.agents[i]].translate(action)\n",
        "    return self.agent_mapping[self.agents[i]].position\n",
        "\n",
        "  def render(self, mode=\"human\"):\n",
        "    \"\"\"\n",
        "    Renders the environment. In human mode, it can print to terminal, open\n",
        "    up a graphical window, or open up some other display that a human can see and understand.\n",
        "    \"\"\"\n",
        "    # if len(self.agents) == 2:\n",
        "    #     string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
        "    #         MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
        "    #     )\n",
        "    # else:\n",
        "    #     string = \"Game over\"\n",
        "    # print(string)\n",
        "    pass\n",
        "\n",
        "  def observe(self, agent):\n",
        "    \"\"\"\n",
        "    Observe should return the observation of the specified agent. This function\n",
        "    should return a sane observation (though not necessarily the most up to date possible)\n",
        "    at any time after reset() is called.\n",
        "    \"\"\"\n",
        "    # observation of one agent is the previous state of the other\n",
        "    return np.array(self.observations[agent])\n",
        "\n",
        "  def close(self):\n",
        "    \"\"\"\n",
        "    Close should release any graphical displays, subprocesses, network connections\n",
        "    or any other environment data which should not be kept around after the\n",
        "    user is no longer using the environment.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "    \"\"\"\n",
        "    Reset needs to initialize the following attributes\n",
        "    - agents\n",
        "    - rewards\n",
        "    - _cumulative_rewards\n",
        "    - dones\n",
        "    - infos\n",
        "    - agent_selection\n",
        "    And must set up the environment so that render(), step(), and observe()\n",
        "    can be called without issues.\n",
        "\n",
        "    Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by step() and observe()\n",
        "    \"\"\"\n",
        "\n",
        "    self.agents = self.possible_agents[:]\n",
        "    self.rewards = {agent: 0 for agent in self.agents}\n",
        "    self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
        "    self.dones = {agent: False for agent in self.agents}\n",
        "    self.infos = {agent: {} for agent in self.agents}\n",
        "    self.state =  {agent: np.zeros(3).astype(np.float32) for agent in self.agents}\n",
        "    self.observations =  {agent: np.zeros(3).astype(np.float32) for agent in self.agents}\n",
        "    self.rounds = 0\n",
        "    self.limited = np.ones((len(self.agents))).astype(np.float32)\n",
        "    self.best_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "    self.prev_sim_matrix = np.zeros(self.num_cameras).astype(np.float32)\n",
        "    \"\"\"\n",
        "    Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
        "    \"\"\"\n",
        "    self._agent_selector = agent_selector(self.agents)\n",
        "    self.agent_selection = self._agent_selector.next()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    step(action) takes in an action for the current agent (specified by\n",
        "    agent_selection) and needs to update\n",
        "    - rewards\n",
        "    - _cumulative_rewards (accumulating the rewards)\n",
        "    - dones\n",
        "    - infos\n",
        "    - agent_selection (to the next agent)\n",
        "    And any internal state used by observe() or render()\n",
        "    \"\"\"\n",
        "\n",
        "    if self.dones[self.agent_selection]:\n",
        "        # handles stepping an agent which is already done\n",
        "        # accepts a None action for the one agent, and moves the agent_selection to\n",
        "        # the next done agent,  or if there are no more done agents, to the next live agent\n",
        "        return self._was_done_step(action)\n",
        "    \n",
        "    agent = self.agent_selection\n",
        "\n",
        "    # the agent which stepped last had its _cumulative_rewards accounted for\n",
        "    # (because it was returned by last()), so the _cumulative_rewards for this\n",
        "    # agent should start again at 0\n",
        "    self._cumulative_rewards[agent] = 0\n",
        "\n",
        "    # stores action of current agent\n",
        "    self.state[agent] = self.take_action(self.possible_agents.index(agent), action)\n",
        "\n",
        "    # collect reward if it is the last agent to act\n",
        "    if self._agent_selector.is_last():\n",
        "      # rewards for all agents are placed in the .rewards dictionary\n",
        "      self.rounds +=1 \n",
        "      self.dones = {agent: self.rounds >= self.limit for agent in self.agents}\n",
        "\n",
        "      # observe the current state\n",
        "      for i in self.agents:\n",
        "        self.observations[i] = {\n",
        "          self.agents[i]: self.agent_mapping[self.agents[i]].position for i in range(len(self.agents))\n",
        "      }\n",
        "\n",
        "      sim_matrix = np.asarray(self.clip_scores()).astype(np.float32)\n",
        "\n",
        "      overall_reward = self.get_reward(sim_matrix)\n",
        "\n",
        "      self.rewards = { self.agents[i]: overall_reward * self.limited[i] for i in range(len(self.agents)) }\n",
        "\n",
        "      self.limited = np.ones((len(self.agents))).astype(np.float32)\n",
        "    else:\n",
        "      # necessary so that observe() returns a reasonable observation at all times.\n",
        "      self.state[self.agents[1 - self.possible_agents.index(agent)]] = None\n",
        "      # no rewards are allocated until both players give an action\n",
        "      self._clear_rewards()\n",
        "\n",
        "    # selects the next agent.\n",
        "    self.agent_selection = self._agent_selector.next()\n",
        "    # Adds .rewards to ._cumulative_rewards\n",
        "    self._accumulate_rewards()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_mesh = load_mesh(f\"{DATA_DIR}/fruit_mesh/pear_export.obj\")\n",
        "table_mesh = load_mesh(f\"{DATA_DIR}/table_mesh/GenericClassicTable001.obj\")"
      ],
      "metadata": {
        "id": "gPNZxSeKR0fA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_object = SceneObject(fruit_mesh, scale=0.25)\n",
        "fruit_objectB = SceneObject(fruit_mesh, scale=0.25)\n",
        "table_object = SceneObject(table_mesh, scale=8)\n",
        "\n",
        "table_object.position=[0,0,0]\n",
        "fruit_objectB.position=[0,0,0]\n",
        "fruit_object.position=[0,0,0]"
      ],
      "metadata": {
        "id": "TxdCqbifR11k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RenderEnv([fruit_object,fruit_objectB, table_object], \"Pieces of fruit on top of a wooden table\", limit=50)"
      ],
      "metadata": {
        "id": "tFI4dLxhSK2U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "count = 0\n",
        "for agent in env.agent_iter():\n",
        "    count +=1\n",
        "    actual_count = int(count / 3)\n",
        "    observation, reward, done, info = env.last()\n",
        "    action = Discrete(len(env.actions_map)).sample()\n",
        "    if done:\n",
        "      action = None\n",
        "    env.step(action)\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "id": "B2tW5VDCUfZ8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_grid(env.best[\"images\"], rows=1, cols=4, rgb=True)"
      ],
      "metadata": {
        "id": "ao8pQfstaaYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_batch_individually(env.scene.scene[0])"
      ],
      "metadata": {
        "id": "yw9ShChXalDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "env.reset()\n",
        "cuda.empty_cache()"
      ],
      "metadata": {
        "id": "4JnvcoRNSLsM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_envs = 1\n",
        "v_env = ss.gym_vec_env_v0(env, num_envs, multiprocessing=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "WdUbGnOofPsZ",
        "outputId": "918da90e-8db5-4894-ad76-021ed937a0e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/supersuit/vector/vector_constructors.py:19: UserWarning: gym_vec_env took in an environment which does not inherit from gym.Env. Note that gym_vec_env only takes in gym-style environments, not pettingzoo environments.\n",
            "  f\"{fn_name} took in an environment which does not inherit from gym.Env. Note that gym_vec_env only takes in gym-style environments, not pettingzoo environments.\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-018b06dd826f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym_vec_env_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/supersuit/vector/vector_constructors.py\u001b[0m in \u001b[0;36mgym_vec_env_v0\u001b[0;34m(env, num_envs, multiprocessing)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncVectorEnv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSyncVectorEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/vector/sync_vector_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns, observation_space, action_space, copy, new_step_api)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mobservation_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mnew_step_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         )\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/vector/vector_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_envs, observation_space, action_space, new_step_api)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_vector_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/vector/utils/spaces.py\u001b[0m in \u001b[0;36mbatch_space\u001b[0;34m(space, n)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[1;32m     41\u001b[0m     raise ValueError(\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;34mf\"Cannot batch space with type `{type(space)}`. The space must be a valid `gym.Space` instance.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot batch space with type `<class 'method'>`. The space must be a valid `gym.Space` instance."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "n_envs = 1\n",
        "n_steps = 64\n",
        "total_timesteps = (n_steps * n_envs) * 1\n",
        "train_model = PPO('MlpPolicy', v_env, verbose=1, n_steps=n_steps).learn(n_eval_episodes=64, total_timesteps=total_timesteps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "VeUCxN7dhbbR",
        "outputId": "4fadea97-b482-4da3-f081-3f3550310464"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8824bd047446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiDiscrete\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiBinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             ),\n\u001b[1;32m    127\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, tensorboard_log, create_eval_env, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtensorboard_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorboard_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0msupported_action_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupported_action_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, tensorboard_log, verbose, device, support_multi_env, create_eval_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_make_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_wrap_env\u001b[0;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrapping the env in a DummyVecEnv.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# Make sure that dict-spaces are not nested (not supported)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mVecEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mobs_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_space_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/util.py\u001b[0m in \u001b[0;36mobs_space_info\u001b[0;34m(obs_space)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}